{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 6 \"Transfer Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: __Иванов Иван Иванович__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагается поучаствовать в конкурсе https://www.kaggle.com/c/human-protein-atlas-image-classification \n",
    "\n",
    "Для зачета требуется получить значение f-меры на leaderboard не меньше 0.25 и прислать ноутбук с кодом и кратким отчетом: что пробовали, что сделали, мысли почему окончательная архитектура лучше остальных.\n",
    "\n",
    "Называйте команду или своего юзера с суффиксом [sphere].\n",
    "\n",
    "Также первые 3 человека получат бонусные 5, 3, 1 балл соответственно. (deadline: 23:59 19 ноября 2018). Скорее всего будут дополнительные плюшки для призеров конкурса.\n",
    "\n",
    "При работе на сервере используйте данные из папки /mnt/disk/exch/human-protein-atlas-image-classification . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### У kaggle есть удобное api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s.evstifeev/anaconda3/bin/pip\", line 7, in <module>\n",
      "    from pip._internal import main\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/__init__.py\", line 40, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/cli/autocompletion.py\", line 8, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/cli/cmdoptions.py\", line 20, in <module>\n",
      "    from pip._internal.utils.hashes import STRONG_HASHES\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/utils/hashes.py\", line 10, in <module>\n",
      "    from pip._internal.utils.misc import read_chunks\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_internal/utils/misc.py\", line 21, in <module>\n",
      "    from pip._vendor import pkg_resources\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3112, in <module>\n",
      "    @_call_aside\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3096, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3140, in _initialize_master_working_set\n",
      "    for dist in working_set\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3140, in <genexpr>\n",
      "    for dist in working_set\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2645, in activate\n",
      "    for pkg in self._get_metadata('namespace_packages.txt'):\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2635, in _get_metadata\n",
      "    for line in self.get_metadata_lines(name):\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1411, in get_metadata_lines\n",
      "    return yield_lines(self.get_metadata(name))\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1407, in get_metadata\n",
      "    value = self._get(self._fn(self.egg_info, name))\n",
      "  File \"/home/s.evstifeev/anaconda3/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1520, in _get\n",
      "    return stream.read()\n",
      "KeyboardInterrupt\n",
      "/usr/bin/sh: kaggle: command not found\n",
      "/usr/bin/sh: kaggle: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle # use --user when working on server\n",
    "## replace ~/.kaggle/kaggle.json with a file from your kaggle profile page\n",
    "## Download data\n",
    "!kaggle competitions download -c human-protein-atlas-image-classification\n",
    "## Sumbit \n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f submit.csv -m \"submition description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import resnet50\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.nn import CrossEntropyLoss, Sequential, Linear, Sigmoid, Tanh, BCELoss, Softmax, BatchNorm1d\n",
    "from torch.utils.data.sampler import Sampler, SubsetRandomSampler, WeightedRandomSampler\n",
    "from PIL import Image # Replace by accimage when ready\n",
    "from PIL.Image import FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM, ROTATE_90, ROTATE_180, ROTATE_270\n",
    "from PIL.ImageEnhance import Color, Contrast, Brightness, Sharpness\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/mnt/disk/exch/human-protein-atlas-image-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ = pd.read_csv(PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSampler(Sampler):\n",
    "     \"\"\"Samples elements from a given list of indices.\n",
    " \n",
    "     Arguments:\n",
    "         indices (list): a list of indices\n",
    "     \"\"\"\n",
    " \n",
    "     def __init__(self, indices):\n",
    "        self.num_samples = len(indices)\n",
    "        self.indices = indices\n",
    " \n",
    "     def __iter__(self):\n",
    "        return iter(self.indices)\n",
    " \n",
    "     def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class MultilabelDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None, train=True):\n",
    "    \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        assert self.df['Id'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X = self.df['Id']\n",
    "        if train:\n",
    "            self.y = self.mlb.fit_transform(self.df['Target'].str.split()).astype(np.float32)\n",
    "        else:\n",
    "            self.y = self.df['Target']\n",
    "\n",
    "    def X(self):\n",
    "        return self.X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def getLabelEncoder(self):\n",
    "        return self.mlb\n",
    "    \n",
    "    def getDF(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = MultilabelDataset(PATH + \"train.csv\", PATH + \"train/\" ,'_green.png',\n",
    "                                 data_transforms['train']\n",
    "                                 )\n",
    "X_val = MultilabelDataset(PATH + \"train.csv\", PATH + \"train/\" ,'_green.png',\n",
    "                                 data_transforms['val']\n",
    "                                 )\n",
    "X_val.mlb = X_train.mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(np.array(range(len(X_train))), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetSampler(valid_idx)\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(X_train,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(X_val,\n",
    "                      batch_size=batch_size,\n",
    "                      sampler=valid_sampler,\n",
    "                      num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/s.evstifeev/.torch/models/resnet50-19c8e357.pth\n",
      "100%|██████████| 102502400/102502400 [00:01<00:00, 53202264.97it/s]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = resnet50(pretrained=True)\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = pretrained_model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замените ``pretrained_model.fc`` на новую полносвязную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_model.fc = Sequential(nn.Linear(num_ftrs, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "if use_cuda:\n",
    "    pretrained_model = pretrained_model.cuda(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype=torch.FloatTensor\n",
    "\n",
    "def train(network, epochs, learning_rate, train_dataloader, test_dataloader, loss=BCELoss(), optim=torch.optim.Adam):\n",
    "    train_loss_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    optimizer = optim(network.parameters(), lr=learning_rate)\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for X, y in train_dataloader:\n",
    "                if use_cuda:\n",
    "                    X = Variable(X).cuda(3)\n",
    "                    y = Variable(y).cuda(3)\n",
    "                else:\n",
    "                    X = Variable(X)\n",
    "                    y = Variable(y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                prediction = network(X)\n",
    "                loss_batch = loss(prediction, y)\n",
    "                losses.append(loss_batch.data[0].cpu())\n",
    "\n",
    "                loss_batch.backward()\n",
    "                optimizer.step()\n",
    "  \n",
    "            train_loss_epochs.append(np.mean(losses))\n",
    "            losses = []    \n",
    "            for X, y in test_dataloader:\n",
    "                if use_cuda:\n",
    "                    X = Variable(X).cuda(3)\n",
    "                    y = Variable(y).cuda(3)\n",
    "                else:\n",
    "                    X = Variable(X)\n",
    "                    y = Variable(y)\n",
    "                \n",
    "                prediction = network(X)\n",
    "                loss_batch = loss(prediction, y)\n",
    "                losses.append(loss_batch.data[0].cpu())\n",
    "                \n",
    "            test_loss_epochs.append(np.mean(losses))\n",
    "            sys.stdout.write('\\rEpoch {0}... (Train/Test) BCE: {1:.3f}/{2:.3f}'.format(\n",
    "                        epoch, train_loss_epochs[-1], test_loss_epochs[-1]))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(train_loss_epochs[1:], label='Train')\n",
    "    plt.plot(test_loss_epochs[1:], label='Test')\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend(loc=0, fontsize=16)\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(pretrained_model, ?, ?, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_validation = []\n",
    "targets = []\n",
    "for X, y in valid_loader:\n",
    "    if use_cuda:\n",
    "        X = Variable(X).cuda(3)\n",
    "    else:\n",
    "        X = Variable(X)\n",
    "   \n",
    "    prediction = pretrained_model(X)\n",
    "    predictions_validation.append(prediction.data.cpu().numpy())\n",
    "    targets.append(y.cpu().numpy())\n",
    "\n",
    "predictions_validation = np.concatenate(preds)\n",
    "targets = np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find optimal threshold for probabilities\n",
    "threshold = ?\n",
    "print (f1_score(targets, (predictions_validation > threshold).astype(int), average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "csv_sub.columns = [\"Id\", \"Target\"]\n",
    "csv_sub.to_csv(\"data/sample_submission_for_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = MultilabelDataset('data/sample_submission_for_dataset.csv','data/test/','_green.png',\n",
    "                                  data_transforms['val'], train=False\n",
    "                                 )\n",
    "X_test.mlb = X_train.mlb\n",
    "test_loader = DataLoader(X_test,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for X, _ in test_loader:\n",
    "    if use_cuda:\n",
    "        X = Variable(X).cuda(3)\n",
    "    else:\n",
    "        X = Variable(X)\n",
    "    \n",
    "    prediction = pretrained_model(X)\n",
    "    predictions.append(prediction.data.cpu().numpy())\n",
    "\n",
    "predictions_test = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_sub = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_sub[\"Predicted\"] = list(map(lambda t: \" \".join([str(x) for x in t]), [list(map(int, t)) for t in X_train.mlb.inverse_transform((predictions_test > threshold))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_sub.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
